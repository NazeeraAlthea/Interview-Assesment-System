{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec6e7bb",
   "metadata": {},
   "source": [
    "# ASR + Speaker Diarization + WER/CER + JSON Output\n",
    "\n",
    "Pipeline ini melakukan:\n",
    "1. Konversi video ke WAV (16kHz mono)\n",
    "2. ASR dengan Whisper\n",
    "3. Speaker diarization dengan `pyannote/speaker-diarization-3.1`\n",
    "4. Penggabungan kata + speaker jadi kalimat lengkap dengan timestamp\n",
    "5. Evaluasi WER & CER menggunakan dataset dari Hugging Face\n",
    "6. Menyimpan output akhir dalam format JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a06fa",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c403ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "import whisper\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pyannote.audio import Pipeline\n",
    "from jiwer import wer, cer\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5caea",
   "metadata": {},
   "source": [
    "## 2. Konfigurasi Utama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ca89725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path file utama (video / audio)\n",
    "FILE_PATH = '../../data/interview_question_1.webm'  # ganti sesuai kebutuhan\n",
    "WAV_PATH = FILE_PATH.rsplit('.', 1)[0] + '.wav'\n",
    "\n",
    "# Model Whisper dan device\n",
    "MODEL_SIZE = 'base.en'  # misal: tiny, base, small, medium\n",
    "DEVICE = 'cpu'          # atau 'cuda' kalau ada GPU\n",
    "\n",
    "# Konfigurasi evaluasi WER/CER\n",
    "DATASET_DIR = \"../../data/openlsr\"  # ganti sesuai foldermu\n",
    "MAX_SAMPLES = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4f4dc",
   "metadata": {},
   "source": [
    "## 3. Fungsi Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4605ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Konversi detik ke format H:M:S,ms (00:00:00,000).\"\"\"\n",
    "    h = int(seconds // 3600)\n",
    "    m = int((seconds % 3600) // 60)\n",
    "    s = int(seconds % 60)\n",
    "    ms = int((seconds - int(seconds)) * 1000)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
    "\n",
    "def ensure_wav_16k_mono(input_path: str) -> str:\n",
    "    \"\"\"Pastikan ada file WAV 16kHz mono. Jika belum ada, konversi dengan ffmpeg.\"\"\"\n",
    "    wav_path = input_path.rsplit('.', 1)[0] + '.wav'\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(f'Mengonversi {input_path} ke {wav_path} (16kHz, mono)...')\n",
    "        subprocess.run([\n",
    "            'ffmpeg', '-y', '-i', input_path,\n",
    "            '-ar', '16000', '-ac', '1', wav_path\n",
    "        ], check=True)\n",
    "    else:\n",
    "        print(f'File WAV sudah ada: {wav_path}')\n",
    "    return wav_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f01cd1",
   "metadata": {},
   "source": [
    "## 4. Load Model Whisper & Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ba96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat pipeline diarisasi 'pyannote/speaker-diarization-3.1'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:43: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline diarisasi berhasil dimuat.\n",
      "Memuat model Whisper 'base.en' di device 'cpu'...\n",
      "Model Whisper berhasil dimuat.\n"
     ]
    }
   ],
   "source": [
    "def load_diarization_pipeline(device: str = 'cpu'):\n",
    "    \"\"\"Load diarization pipeline PyAnnote 3.x.\"\"\"\n",
    "    print(\"Memuat pipeline diarisasi 'pyannote/speaker-diarization-3.1'...\")\n",
    "\n",
    "    # karena kamu sudah huggingface-cli login, tidak perlu token di sini\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\"\n",
    "    )\n",
    "\n",
    "    pipeline.to(torch.device(device))\n",
    "    print(\"Pipeline diarisasi berhasil dimuat.\")\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def load_whisper_model(model_size: str = \"base.en\", device: str = \"cpu\"):\n",
    "    \"\"\"Load Whisper model.\"\"\"\n",
    "    print(f\"Memuat model Whisper '{model_size}' di device '{device}'...\")\n",
    "\n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "\n",
    "    print(\"Model Whisper berhasil dimuat.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Eksekusi\n",
    "diarization_pipeline = load_diarization_pipeline(DEVICE)\n",
    "whisper_model = load_whisper_model(MODEL_SIZE, DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04fc28",
   "metadata": {},
   "source": [
    "## 5. Proses Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f321312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\05_Personal\\Asah by Dicoding\\capstone-project\\src\\asr\n",
      "Trying to load: ../../data/interview_question_1.webm\n",
      "Exists? True\n",
      "File WAV sudah ada: ../../data/interview_question_1.wav\n",
      "\n",
      "Menjalankan diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diarization selesai dalam 40.68 detik.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.364094</td>\n",
       "      <td>6.342219</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.169094</td>\n",
       "      <td>11.725344</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.024719</td>\n",
       "      <td>25.326594</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.967844</td>\n",
       "      <td>31.671594</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.072219</td>\n",
       "      <td>33.882219</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start        end     speaker\n",
       "0   1.364094   6.342219  SPEAKER_00\n",
       "1   7.169094  11.725344  SPEAKER_00\n",
       "2  13.024719  25.326594  SPEAKER_00\n",
       "3  25.967844  31.671594  SPEAKER_00\n",
       "4  33.072219  33.882219  SPEAKER_00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Trying to load:\", FILE_PATH)\n",
    "print(\"Exists?\", os.path.exists(FILE_PATH))\n",
    "\n",
    "\n",
    "wav_path = ensure_wav_16k_mono(FILE_PATH)\n",
    "\n",
    "print('\\nMenjalankan diarization...')\n",
    "start_diar = time.time()\n",
    "diarization_result = diarization_pipeline(wav_path)\n",
    "end_diar = time.time()\n",
    "print(f'Diarization selesai dalam {end_diar - start_diar:.2f} detik.')\n",
    "\n",
    "speaker_turns = []\n",
    "for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "    speaker_turns.append({\n",
    "        'start': turn.start,\n",
    "        'end': turn.end,\n",
    "        'speaker': speaker\n",
    "    })\n",
    "\n",
    "speaker_df = pd.DataFrame(speaker_turns)\n",
    "speaker_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5e049",
   "metadata": {},
   "source": [
    "## 6. Proses ASR dengan Whisper (Word Timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed9c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menjalankan transkripsi Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transkripsi selesai dalam 9.05 detik.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'segments', 'language'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nMenjalankan transkripsi Whisper...')\n",
    "start_asr = time.time()\n",
    "asr_result = whisper_model.transcribe(\n",
    "    FILE_PATH,\n",
    "    language='en',\n",
    "    word_timestamps=True\n",
    ")\n",
    "end_asr = time.time()\n",
    "print(f'Transkripsi selesai dalam {end_asr - start_asr:.2f} detik.')\n",
    "\n",
    "asr_result.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb6556",
   "metadata": {},
   "source": [
    "## 7. Menggabungkan Kata + Speaker menjadi Kalimat Bertimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8227b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total kata: 123\n",
      "Total segmen kalimat: 6\n",
      "[00:00:00,740 --> 00:00:01,419] UNKNOWN:  Can\n",
      "[00:00:01,419 --> 00:00:06,160] SPEAKER_00:  you  share  any  specific  challenges  you  face  when  working  on  certification  and  how  you  are  coming  in?\n",
      "[00:00:06,879 --> 00:00:07,440] UNKNOWN:  Ah,\n",
      "[00:00:07,820 --> 00:00:54,100] SPEAKER_00:  okay  actually,  for  these  challenges,  there  are  some  challenges  when  I  took  the  certifications,  especially  for  the  projects  I  mentioned  that  I  already  working  with  it.  The  first  one  is  actually  to  meet  the  specific  accuracy  or  the  calculation  loss  for  the  application  matrix.  Actually,  that's  just  a  need  to  take  some  trial  and  error  with\n",
      "[00:00:54,100 --> 00:00:55,219] UNKNOWN:  different\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for seg in asr_result.get('segments', []):\n",
    "    for w in seg.get('words', []):\n",
    "        all_words.append(w)\n",
    "\n",
    "print(f'Total kata: {len(all_words)}')\n",
    "\n",
    "word_speaker_mapping = []\n",
    "for word in all_words:\n",
    "    w_start = word['start']\n",
    "    match = speaker_df[(speaker_df['start'] <= w_start) & (speaker_df['end'] >= w_start)]\n",
    "    if not match.empty:\n",
    "        spk = match.iloc[0]['speaker']\n",
    "    else:\n",
    "        spk = 'UNKNOWN'\n",
    "    word_speaker_mapping.append({\n",
    "        'start': word['start'],\n",
    "        'end': word['end'],\n",
    "        'word': word['word'],\n",
    "        'speaker': spk\n",
    "    })\n",
    "\n",
    "final_segments = []\n",
    "current = None\n",
    "\n",
    "for w in word_speaker_mapping:\n",
    "    if current is None:\n",
    "        current = {\n",
    "            'start': w['start'],\n",
    "            'end': w['end'],\n",
    "            'speaker': w['speaker'],\n",
    "            'text': w['word']\n",
    "        }\n",
    "    else:\n",
    "        if w['speaker'] == current['speaker']:\n",
    "            if not current['text'].endswith(' '):\n",
    "                current['text'] += ' '\n",
    "            current['text'] += w['word']\n",
    "            current['end'] = w['end']\n",
    "        else:\n",
    "            final_segments.append(current)\n",
    "            current = {\n",
    "                'start': w['start'],\n",
    "                'end': w['end'],\n",
    "                'speaker': w['speaker'],\n",
    "                'text': w['word']\n",
    "            }\n",
    "\n",
    "if current is not None:\n",
    "    final_segments.append(current)\n",
    "\n",
    "print(f'Total segmen kalimat: {len(final_segments)}')\n",
    "\n",
    "for seg in final_segments[:5]:\n",
    "    print(f\"[{format_time(seg['start'])} --> {format_time(seg['end'])}] {seg['speaker']}: {seg['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1591ab",
   "metadata": {},
   "source": [
    "## 8. Evaluasi WER & CER dengan Dataset Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4951f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# =========================================================\n",
    "# NORMALISASI TEKS (WAJIB BIAR WER REALISTIC)\n",
    "# =========================================================\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)    # hapus punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)       # rapikan spasi\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df9d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# HITUNG TOTAL FILE UNTUK PROGRESS BAR\n",
    "# =========================================================\n",
    "def count_total_samples(root_dir):\n",
    "    total = 0\n",
    "    for speaker in os.listdir(root_dir):\n",
    "        spk_path = os.path.join(root_dir, speaker)\n",
    "        if not os.path.isdir(spk_path):\n",
    "            continue\n",
    "        for chapter in os.listdir(spk_path):\n",
    "            trans_file = os.path.join(spk_path, chapter, f\"{speaker}-{chapter}.trans.txt\")\n",
    "            if os.path.exists(trans_file):\n",
    "                with open(trans_file, \"r\", encoding=\"utf8\") as f:\n",
    "                    total += len(f.readlines())\n",
    "    return total\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# EVALUASI FULL DATASET TANPA max_samples\n",
    "# =========================================================\n",
    "def evaluate_dataset_full(root_dir, whisper_model):\n",
    "    refs = []\n",
    "    hyps = []\n",
    "\n",
    "    print(f\"\\nEvaluasi full dataset lokal: {root_dir}\\n\")\n",
    "\n",
    "    total_samples = count_total_samples(root_dir)\n",
    "    print(f\"Total sampel ditemukan: {total_samples}\\n\")\n",
    "\n",
    "    pbar = tqdm(total=total_samples, desc=\"Processing\")\n",
    "\n",
    "    # LOOP UTAMA\n",
    "    for speaker in sorted(os.listdir(root_dir)):\n",
    "        spk_dir = os.path.join(root_dir, speaker)\n",
    "        if not os.path.isdir(spk_dir):\n",
    "            continue\n",
    "\n",
    "        for chapter in sorted(os.listdir(spk_dir)):\n",
    "            chap_dir = os.path.join(spk_dir, chapter)\n",
    "\n",
    "            # file transkripsi\n",
    "            trans_file = os.path.join(chap_dir, f\"{speaker}-{chapter}.trans.txt\")\n",
    "            if not os.path.exists(trans_file):\n",
    "                continue\n",
    "\n",
    "            with open(trans_file, \"r\", encoding=\"utf8\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            # PROSES SETIAP UTTERANCE\n",
    "            for line in lines:\n",
    "                pbar.update(1)\n",
    "\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                utt_id = parts[0]\n",
    "                ref_text = \" \".join(parts[1:])\n",
    "                audio_path = os.path.join(chap_dir, f\"{utt_id}.flac\")\n",
    "\n",
    "                if not os.path.exists(audio_path):\n",
    "                    print(f\"Audio missing: {audio_path}\")\n",
    "                    continue\n",
    "\n",
    "                # load audio\n",
    "                audio, sr = librosa.load(audio_path, sr=None)\n",
    "                if sr != 16000:\n",
    "                    audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "                # transkripsi Whisper\n",
    "                result = whisper_model.transcribe(audio, fp16=False)\n",
    "                hyp_text = result.get(\"text\", \"\").strip()\n",
    "\n",
    "                # simpan setelah normalisasi\n",
    "                refs.append(normalize(ref_text))\n",
    "                hyps.append(normalize(hyp_text))\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if len(refs) == 0:\n",
    "        print(\"Tidak ada sample valid ditemukan.\")\n",
    "        return None\n",
    "\n",
    "    # =========================================================\n",
    "    # HITUNG METRIK\n",
    "    # =========================================================\n",
    "    wer_val = wer(refs, hyps)\n",
    "    cer_val = cer(refs, hyps)\n",
    "\n",
    "    print(\"\\n=========================\")\n",
    "    print(\"         FINAL METRICS\")\n",
    "    print(\"=========================\")\n",
    "    print(f\"WER: {wer_val:.4f}  ({wer_val * 100:.2f}%)\")\n",
    "    print(f\"CER: {cer_val:.4f}  ({cer_val * 100:.2f}%)\")\n",
    "    print(f\"Total sampel dievaluasi: {len(refs)}\")\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer_val,\n",
    "        \"wer_percent\": wer_val * 100,\n",
    "        \"cer\": cer_val,\n",
    "        \"cer_percent\": cer_val * 100,\n",
    "        \"num_samples\": len(refs)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a8042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluasi full dataset lokal: ../../data/openlsr\n",
      "\n",
      "Total sampel ditemukan: 2620\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 15/2620 [00:12<40:21,  1.08it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m metrics = \u001b[43mevaluate_dataset_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m metrics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mevaluate_dataset_full\u001b[39m\u001b[34m(root_dir, whisper_model)\u001b[39m\n\u001b[32m     74\u001b[39m     audio = librosa.resample(audio, orig_sr=sr, target_sr=\u001b[32m16000\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# transkripsi Whisper\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m result = \u001b[43mwhisper_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m hyp_text = result.get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# simpan setelah normalisasi\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\transcribe.py:240\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[39m\n\u001b[32m    237\u001b[39m mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n\u001b[32m    239\u001b[39m decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\transcribe.py:170\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    167\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    174\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    175\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    176\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\decoding.py:687\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.sample_len):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    690\u001b[39m             i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.no_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    691\u001b[39m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[32m    692\u001b[39m             probs_at_sot = logits[:, \u001b[38;5;28mself\u001b[39m.sot_index].float().softmax(dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\decoding.py:163\u001b[39m, in \u001b[36mPyTorchInference.logits\u001b[39m\u001b[34m(self, tokens, audio_features)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.initial_token_length:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m     tokens = tokens[:, -\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\model.py:211\u001b[39m, in \u001b[36mTextDecoder.forward\u001b[39m\u001b[34m(self, x, xa, kv_cache)\u001b[39m\n\u001b[32m    208\u001b[39m x = x.to(xa.dtype)\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    214\u001b[39m logits = (\n\u001b[32m    215\u001b[39m     x @ torch.transpose(\u001b[38;5;28mself\u001b[39m.token_embedding.weight.to(x.dtype), \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    216\u001b[39m ).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\model.py:136\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    131\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m     kv_cache: Optional[\u001b[38;5;28mdict\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    135\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    138\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\model.py:91\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m     88\u001b[39m     v = kv_cache[\u001b[38;5;28mself\u001b[39m.value]\n\u001b[32m     90\u001b[39m wv, qk = \u001b[38;5;28mself\u001b[39m.qkv_attention(q, k, v, mask)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwv\u001b[49m\u001b[43m)\u001b[49m, qk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\05_Personal\\Asah by Dicoding\\capstone-project\\venv\\Lib\\site-packages\\whisper\\model.py:37\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "metrics = evaluate_dataset_full(DATASET_DIR, whisper_model)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff767f8",
   "metadata": {},
   "source": [
    "## 9. Menyimpan Hasil Akhir ke JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46c4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON disimpan di: outputs\\asr_diarization_output.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'outputs\\\\asr_diarization_output.json'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {\n",
    "    'file_path': FILE_PATH,\n",
    "    'model_size': MODEL_SIZE,\n",
    "    'device': DEVICE,\n",
    "    'segments': [\n",
    "        {\n",
    "            'start_sec': seg['start'],\n",
    "            'end_sec': seg['end'],\n",
    "            'start_time': format_time(seg['start']),\n",
    "            'end_time': format_time(seg['end']),\n",
    "            'speaker': seg['speaker'],\n",
    "            'text': seg['text']\n",
    "        }\n",
    "        for seg in final_segments\n",
    "    ],\n",
    "    'wer_cer_metrics': metrics\n",
    "}\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "json_path = os.path.join('outputs', 'asr_diarization_output.json')\n",
    "\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'JSON disimpan di: {json_path}')\n",
    "json_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
